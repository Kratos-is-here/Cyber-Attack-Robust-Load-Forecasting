# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eNJZEW0kxOfEY-f1hz2_BBXepXa6g4um
"""

import pandas as pd
import random
import numpy as np

a = pd.read_csv('da.csv')

a.iloc[:]

N_in = 168
gamma_se = 24 * 0 # 0, 24, 48, 72
p_se = 0.15
end = int(N_in - gamma_se)
start = int(end + 1 - N_in * p_se)
scaling_factor = 0.4 # or 2

start

end

a.iloc[start:end, 3]

ramp_rate = 1
# ne = 
# i = 
l_re = 0

a['Scaling_attack'] = a.iloc[:,3]
a['Scaling_attack'] = a.iloc[:,3]

# for i in range(start, end + 1):

a.iloc[start:end, 4] = a.iloc[start:end, 3] * scaling_factor

# for i in range(start, end + 1):
    
#     a.iloc[i, 5] = (1 + l_re * min(abs(i - start), abs(end-i))) * a.iloc[i, 3] 

# a['Scaling_attack'] = a['Total Load'].apply(lambda x : x * scaling_factor)
# a['Ramping_attack'] = a['Total Load'].apply(lambda x : 1 + x * scaling_factor)
# a.iloc[start:end,3] *= scaling_factor

a.to_csv('scaling_attack.csv', index = False)

b = pd.read_csv('scaling_attack.csv')

b['Ramping_attack'] = b.iloc[:,3]
b['RA_labels'] = 0

b

l_re = 1 # 1
b.iloc[:-5 ,5]

b[start-3:end]


for i in range(start, end + 1):
    constt = (1 + l_re * min(abs(i - start), abs(end - i)))
    # print(constt)
    b.iloc[i, 6] = constt * b.iloc[i, 3] 
    b.iloc[i, 7] = 1

l_de = 1.5
p_de = int(b.iloc[:].shape[0] * 0.15)
p_de

b['Random_attack'] = b.iloc[:, 3]
b['RAN_labels'] = 0

b.iloc[:].shape

x = set(random.sample(range(b.iloc[:].shape[0]), p_de))
len(x)

for i in x:
    b.iloc[i, 8] = l_de * b.iloc[i, 3]
    b.iloc[i, 9] = 1

b

b.to_csv('data_attacks.csv', index = False)

b

b[start-3:end]

import numpy as np

a = np.zeros(b.shape[0])
a[start:end] = 1

b['SA_labels'] = a

b.to_csv('scaling_attack.csv', index = False)

!conda install -c conda-forge tqdm

import torch
import torchvision # torch package for vision related things
import torch.nn.functional as F  # Parameterless functions, like (some) activation functions
import torchvision.datasets as datasets  # Standard datasets
import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation
from torch import optim  # For optimizers like SGD, Adam, etc.
from torch import nn  # All neural network modules
from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.
from tqdm import tqdm  
from torch.utils.tensorboard import SummaryWriter

writer1 = SummaryWriter()
writer2 = SummaryWriter()
writer3 = SummaryWriter()
writer4 = SummaryWriter()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = nn.Sequential(
    nn.Linear(2,50),
    nn.Linear(50,1),
    nn.Sigmoid(0),
)

criterion = nn.L1Loss()
optim = optim.SGD(model.parameters(), lr = 0.01)
params = {
    'batch_size' : 32,
    
}

class load_data(DataLoader):
      def __init__(self, csv_file, transform=None,attack='scaling', testing = False):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.testing = testing
        self.dataset = pd.read_csv(csv_file)
        self.transform = transform
        self.attack=attack
      def __len__(self):
        return len(self.dataset)

      def __getitem__(self, idx):
        # if self.attack=='scaling':
        #     c=5
        # if self.attack=='ramping':
        #     c=7
        # if self.attack=='random':
        #     c=9

        data = self.dataset.iloc[idx,1]
        label = self.dataset.iloc[idx,3]

        if self.testing:
          return data
        
        return (data,label)

def run_me(model, data, test_data, params, criterion, optimizer, scaler = writer):
    
    train_loader = data(data, batch_size = params['batch_size'], shuffle = True)
    test_loader = data(test_data, batch_size = params['batch_size'], shuffle = False)
    
    num_epochs = 20000
    
    for epoch in range(num_epochs):
        for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):
            # Get data to cuda if possible
            data = data.to(device=device)
            targets = targets.to(device=device)

            # forward
            scores = model(data)
            loss = criterion(scores, targets)
            scaler.add('training-loss', loss.item(), epoch)
            # backward
            optimizer.zero_grad()
            loss.backward()

            # gradient descent or adam step
            optimizer.step()
    
#  def check_accuracy(loader, model):
#     if loader.dataset.train:
#         print("Checking accuracy on training data")
#     else:
#         print("Checking accuracy on test data")

    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(device=device).squeeze(1)
            y = y.to(device=device)

            scores = model(x)
            _, predictions = scores.max(1)
            num_correct += (predictions == y).sum()
            num_samples += predictions.size(0)

        print(
            f"Got {num_correct} / {num_samples} with accuracy  \
            {float(num_correct)/float(num_samples)*100:.2f}"
        )

    model.train()



run_me(model, load_data, load_data, params, criterion, optim)